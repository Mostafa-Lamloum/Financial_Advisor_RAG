{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3607d086-411c-4f83-819e-0c25b8c77ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "from transformers import DistilBertTokenizer\n",
    "import re\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ac356-6301-404f-ba35-da181afd1af2",
   "metadata": {},
   "source": [
    "# Pinecone connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d86663-68aa-4a18-983e-ed636d7b60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=\"09c53ad3-28c3-4a7f-88fa-68260478262e\")\n",
    "\n",
    "pc.create_index(\n",
    "    name=\"bert-test88\",\n",
    "    dimension=768, # Replace with your model dimensions\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")\n",
    "\n",
    "index = pc.Index(\"bert-test88\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a2724-6a3f-4994-be77-8707d561ebf6",
   "metadata": {},
   "source": [
    "# PDF Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82e3724-4f72-4aac-b89d-03c83b242668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_from_pdf(pdf_path):\n",
    "    document_text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    document_text += page_text + \"\\n\"\n",
    "                else:\n",
    "                    # Perform OCR if no text is found\n",
    "                    img = page.to_image(resolution=300).original\n",
    "                    page_text = pytesseract.image_to_string(img)\n",
    "                    document_text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "\n",
    "    document_length = len(document_text.strip())\n",
    "    print(f\"Document length for {os.path.basename(pdf_path)}: {document_length} characters\")\n",
    "    \n",
    "    return document_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2953c7f5-5175-45fb-bfa1-9537560a440c",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ab3515-d3e3-46b9-9132-75fbff688d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = text.strip()  # Trim leading and trailing spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "368c018d-6a65-4438-a3a7-78225adbd531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_textual_content(text):\n",
    "    lines = text.split('\\n')\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        # This regex matches lines with mostly numbers\n",
    "        if not re.match(r'^(\\d+(\\.\\d+)?[\\s\\t,]*){2,}$', line):\n",
    "            filtered_lines.append(line)\n",
    "    filtered_text = ' '.join(filtered_lines)\n",
    "    filtered_text = re.sub(r'\\s{2,}', ' ', filtered_text)\n",
    "    return preprocess_text(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba01ee-129c-4205-b413-57c39169a3d2",
   "metadata": {},
   "source": [
    "# Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2519831a-d091-4730-873b-878c94416550",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BlueOrangeDigital/distilbert-cross-segment-document-chunking\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad8a52e-4722-425f-a831-7f332f86b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_len=450):\n",
    "    text = preprocess_text(text)\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)  # Sentence splitting\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_len = len(tokenizer.encode(sentence, add_special_tokens=False))\n",
    "\n",
    "        if current_length + sentence_len > max_len:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "\n",
    "        # Check if a single sentence exceeds the maximum length, if so, truncate\n",
    "        if sentence_len > max_len:\n",
    "            sentence = tokenizer.decode(tokenizer.encode(sentence, add_special_tokens=False)[:max_len])\n",
    "            sentence_len = max_len\n",
    "\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += sentence_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4321b50-0b5c-4e57-b6bb-d3d2276d80ad",
   "metadata": {},
   "source": [
    "# Vector Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f001cf10-c61c-45dd-bdc9-0c3ee35e52b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "global counter\n",
    "counter = 1\n",
    "def vector_gen(paragraphs,model_name):\n",
    "\n",
    "    vectors = []\n",
    "    \n",
    "    num_paragraphs = len(paragraphs) \n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "    \n",
    "    for i,text in enumerate(paragraphs):\n",
    "        \n",
    "        inputs  = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "        record_dict = {\"id\": str(counter)+'-'+str(i), \"values\":cls_embedding, \"metadata\":{\"text\":text}}\n",
    "\n",
    "        vectors.append(record_dict)\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc4a98-6145-41ec-af27-75f49efc2c1b",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa88d14-1cd9-4c6e-9dbd-fcfd1bbc9ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting EGPTds.pdf\n",
      "Document length for EGPTds.pdf: 434 characters\n",
      " * Finnished Cleaning\n",
      " * number of chunks = 1\n",
      "EGPTds.pdf Finished\n",
      "Starting ForeignCDs.pdf\n",
      "Document length for ForeignCDs.pdf: 2882 characters\n",
      " * Finnished Cleaning\n",
      " * number of chunks = 1\n",
      "ForeignCDs.pdf Finished\n",
      "Starting ifrs-9-financial-instruments.pdf\n",
      "Error reading ifrs-9-financial-instruments.pdf: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Document length for ifrs-9-financial-instruments.pdf: 486264 characters\n",
      " * Finnished Cleaning\n",
      " * number of chunks = 243\n",
      "ifrs-9-financial-instruments.pdf Finished\n",
      "Starting IstithmarFundFactSheet.pdf\n",
      "Document length for IstithmarFundFactSheet.pdf: 5153 characters\n",
      " * Finnished Cleaning\n",
      " * number of chunks = 4\n",
      "IstithmarFundFactSheet.pdf Finished\n",
      "Starting JPM_Report_May_2024_1714798734_240504_145944.pdf\n",
      "Document length for JPM_Report_May_2024_1714798734_240504_145944.pdf: 47833 characters\n",
      " * Finnished Cleaning\n",
      " * number of chunks = 27\n",
      "JPM_Report_May_2024_1714798734_240504_145944.pdf Finished\n",
      "Starting ssrn-4861479.pdf\n",
      "Document length for ssrn-4861479.pdf: 174466 characters\n",
      " * Finnished Cleaning\n",
      " * number of chunks = 111\n",
      "ssrn-4861479.pdf Finished\n"
     ]
    }
   ],
   "source": [
    "directory = './'\n",
    "pdf_paths = []\n",
    "for file in os.listdir(directory):\n",
    "    \n",
    "    if file.endswith('.pdf'):\n",
    "        print(f\"Starting {file}\")\n",
    "        # Read PDF\n",
    "        document_text = read_text_from_pdf(file)\n",
    "        # CLean PDF by removing white spaces\n",
    "        cleaned_text = preprocess_text(document_text)\n",
    "        # Filter PDF to keep text only and remove any special characters\n",
    "        filtered_text = filter_non_textual_content(cleaned_text)\n",
    "        print(f' * Finnished Cleaning')\n",
    "        # Split the data into chunks\n",
    "        chunks = chunk_text(filtered_text)\n",
    "        print(f' * number of chunks = {len(chunks)}')\n",
    "        # Start Text embeddings\n",
    "        embeddings = vector_gen(chunks, 'bert-base-uncased')\n",
    "        \n",
    "        index.upsert(vectors=embeddings)\n",
    "        print(f\"{file} Finished\")\n",
    "    \n",
    "    counter += 1\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634baae-e5eb-4f3c-a5fe-67012d3fc365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
